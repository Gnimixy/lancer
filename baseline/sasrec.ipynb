{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "seed_everything(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dir\": \"../data/ml-1m\",\n",
    "    \"epochs\": 40,\n",
    "    \"device\": \"cuda:4\",\n",
    "    \"lr\": 1e-4,\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ndjson(input_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        lines = f.read()\n",
    "        d = [json.loads(l) for l in lines.splitlines()]\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_seq_txt(input_file):\n",
    "    output = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip(\"\\n\")\n",
    "            line = line.split(\" \")\n",
    "            line = [int(i) for i in line]\n",
    "            output.append(line)\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_dataset(data_dir, mode=\"\"):\n",
    "    mode_list = [\"\", \"train\", \"dev\", \"test\"]\n",
    "    if mode not in mode_list:\n",
    "        raise ValueError(\"Incorrect mode. Must be `train`|`dev`|`test`.\")\n",
    "\n",
    "    if mode != \"\":\n",
    "        data_dir = os.path.join(data_dir, mode)\n",
    "    behavior = load_seq_txt(os.path.join(data_dir, \"sequential_data.txt\"))\n",
    "    content = load_ndjson(os.path.join(data_dir, \"content.json\"))\n",
    "    return behavior, content\n",
    "\n",
    "\n",
    "behavior, content = load_dataset(args.data_dir, mode=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Dataset(Dataset):\n",
    "    def __init__(self, purchase_history, mode=\"\") -> None:\n",
    "        self.max_len = 10\n",
    "        self.purchase_history = purchase_history\n",
    "        self.mode = mode\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.purchase_history)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        purchase_history = self.purchase_history[index]\n",
    "        if self.mode == \"train\":\n",
    "            purchase_history = purchase_history[0:-1]\n",
    "        \n",
    "        seq_list = purchase_history[0:-1]\n",
    "        tgt_list = purchase_history[1:]\n",
    "        seq = self.truncate_and_pad(seq_list)\n",
    "        tgt = self.truncate_and_pad(tgt_list)\n",
    "        return torch.LongTensor(seq), torch.LongTensor(tgt)\n",
    "    \n",
    "    def truncate_and_pad(self, input_list):\n",
    "        length = len(input_list)\n",
    "        if length > self.max_len:\n",
    "            return input_list[length - self.max_len : length]\n",
    "        elif length < self.max_len:\n",
    "            return [0] * (self.max_len - length) + input_list\n",
    "        else:\n",
    "            return input_list\n",
    "\n",
    "\n",
    "max_item = 0\n",
    "def process_dataset(behavior, content):\n",
    "    purchase_history, impression_items, impression_labels = [], [], []\n",
    "    global max_item\n",
    "    for i, user_info in enumerate(behavior):\n",
    "        if not user_info or len(user_info) < 5:\n",
    "            continue\n",
    "        user_info = user_info[1:]\n",
    "        max_item = max(max_item, max(user_info))\n",
    "        purchase_history.append(user_info)\n",
    "    return purchase_history\n",
    "\n",
    "\n",
    "train_dataset = GRU_Dataset(process_dataset(behavior, content), mode=\"train\")\n",
    "test_dataset = GRU_Dataset(process_dataset(behavior, content), mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_, uniform_, constant_\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.pe.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, enable_res_parameter, dropout=0.1):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.enable = enable_res_parameter\n",
    "        if enable_res_parameter:\n",
    "            self.a = nn.Parameter(torch.tensor(1e-8))\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        if not self.enable:\n",
    "            return self.norm(x + self.dropout(sublayer(x)))\n",
    "        else:\n",
    "            return self.norm(x + self.dropout(self.a * sublayer(x)))\n",
    "\n",
    "\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    FFN implement\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ffn, dropout=0.1):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.linear2(self.activation(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    TRM layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, attn_heads, d_ffn, enable_res_parameter, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = MultiHeadAttention(attn_heads, d_model, dropout)\n",
    "        self.ffn = PointWiseFeedForward(d_model, d_ffn, dropout)\n",
    "        self.skipconnect1 = SublayerConnection(d_model, enable_res_parameter, dropout)\n",
    "        self.skipconnect2 = SublayerConnection(d_model, enable_res_parameter, dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.skipconnect1(x, lambda _x: self.attn.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.skipconnect2(x, self.ffn)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn.init import xavier_normal_, uniform_, constant_\n",
    "\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(M, self).__init__()\n",
    "        self.num_item = args.num_item + 1\n",
    "        self.device = args.device\n",
    "        d_model = args.d_model\n",
    "        attn_heads = args.attn_heads\n",
    "        d_ffn = args.d_ffn\n",
    "        layers = args.bert_layers\n",
    "        dropout = args.dropout\n",
    "        self.max_len = args.max_len\n",
    "        enable_res_parameter = args.enable_res_parameter\n",
    "        self.dense_vis = nn.Linear(2048,d_model)\n",
    "        self.token = nn.Embedding(self.num_item, d_model)\n",
    "        self.attention_mask = torch.tril(torch.ones((self.max_len, self.max_len), dtype=torch.bool)).to(self.device)\n",
    "        self.attention_mask.requires_grad = False\n",
    "        self.TRMs_id = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, attn_heads, d_ffn, enable_res_parameter, dropout) for i in range(layers)])\n",
    "        self.position = PositionalEmbedding(self.max_len, d_model)\n",
    "        self.pred = nn.Linear(d_model,self.num_item)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            stdv = np.sqrt(1. / self.num_item)\n",
    "            uniform_(module.weight.data, -stdv, stdv)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            xavier_normal_(module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                constant_(module.bias.data, 0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        id_embd = self.token(x) + self.position(x)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        mask *= self.attention_mask\n",
    "        for TRM in self.TRMs_id:\n",
    "            id_embd = TRM(id_embd, mask)\n",
    "        pre = self.pred(id_embd)\n",
    "        return pre\n",
    "\n",
    "sasrec_args = {\n",
    "    \"num_item\": max_item,\n",
    "    \"device\": \"cuda:4\",\n",
    "    \"d_model\": 128,\n",
    "    \"attn_heads\": 4,\n",
    "    \"d_ffn\": 512,\n",
    "    \"bert_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_len\": 10,\n",
    "    \"enable_res_parameter\": 0,\n",
    "}\n",
    "model = M(argparse.Namespace(**sasrec_args))\n",
    "\n",
    "def print_model_parm_nums(model):\n",
    "    total = sum([param.nelement() for param in model.parameters()])\n",
    "    print('  + Number of params: %.2fM' % (total / 1e6))\n",
    "    \n",
    "print_model_parm_nums(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dcg_score(y_true, order, k=10):\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def _ndcg_score(y_true, order, k=10):\n",
    "    actual = _dcg_score(y_true, order, k)\n",
    "    return actual / 1.\n",
    "\n",
    "\n",
    "class MetricScores(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.ndcg5s = []\n",
    "        self.ndcg10s = []\n",
    "        self.ndcg20s = []\n",
    "        self.recall1_true = 0\n",
    "        self.recall5_true = 0\n",
    "        self.recall5_total = 0\n",
    "        self.recall10_true = 0\n",
    "        self.recall10_total = 0\n",
    "        self.recall20_true = 0\n",
    "        self.recall20_total = 0\n",
    "        self.k = [1, 5, 10, 20]\n",
    "\n",
    "    def __call__(self, label_ids: torch.Tensor, pred_ids: torch.Tensor):\n",
    "        assert len(label_ids) == len(pred_ids)\n",
    "    \n",
    "        # max_k = max(self.k)\n",
    "        for k in self.k:\n",
    "            pred = pred_ids[:, 0:k].detach().cpu().numpy()\n",
    "            label = label_ids.detach().cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "            binary_labels = np.where(pred==label, 1, 0)\n",
    "\n",
    "            if k == 10:\n",
    "                self.recall10_true += np.sum(binary_labels)\n",
    "                self.recall10_total += binary_labels.shape[0]\n",
    "                for y_true in binary_labels:\n",
    "                    order = [i for i in range(10)]\n",
    "                    ndcg5 = _ndcg_score(y_true, order, 5)\n",
    "                    ndcg10 = _ndcg_score(y_true, order, 10)\n",
    "                    \n",
    "                    self.ndcg5s.append(ndcg5)\n",
    "                    self.ndcg10s.append(ndcg10)\n",
    "            elif k == 5:\n",
    "                self.recall5_true += np.sum(binary_labels)\n",
    "                self.recall5_total += binary_labels.shape[0]\n",
    "            elif k == 1:\n",
    "                self.recall1_true += np.sum(binary_labels)\n",
    "            elif k == 20:\n",
    "                self.recall20_true += np.sum(binary_labels)\n",
    "                self.recall20_total += binary_labels.shape[0]\n",
    "                for y_true in binary_labels:\n",
    "                    order = [i for i in range(20)]\n",
    "                    ndcg20 = _ndcg_score(y_true, order, 20)\n",
    "                    self.ndcg20s.append(ndcg20)\n",
    "        return\n",
    "\n",
    "\n",
    "    def output(self):\n",
    "        ndcg5, ndcg10, ndcg20 = np.mean(self.ndcg5s), np.mean(self.ndcg10s), np.mean(self.ndcg20s)\n",
    "        recall1 = self.recall1_true / self.recall5_total\n",
    "        recall5 = self.recall5_true / self.recall5_total\n",
    "        recall10 = self.recall10_true / self.recall10_total\n",
    "        recall20 = self.recall20_true / self.recall20_total\n",
    "        print(\n",
    "            \"Recall@1: {:.4f}\\nRecall@5: {:.4f}\\nRecall@10: {:.4f}\\nRecall@20: {:.4f}\\nnDCG@5: {:.4f}\\nnDCG@10: {:.4f}\\nnDCG@20: {:.4f}\\n\".format(\n",
    "                recall1, recall5, recall10, recall20, ndcg5, ndcg10, ndcg20\n",
    "            )\n",
    "        )\n",
    "        res = {}\n",
    "        res[\"scores\"] = {\n",
    "            \"Recall@1\": recall1,\n",
    "            \"Recall@5\": recall5,\n",
    "            \"Recall@10\": recall10,\n",
    "            \"Recall@20\": recall20,\n",
    "            \"nDCG@5\": ndcg5,\n",
    "            \"nDCG@10\": ndcg10,\n",
    "            \"nDCG@20\": ndcg20,\n",
    "        }\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, max_item, device):\n",
    "    avg_ndcg, avg_hit, cnt = 0.0, 0.0, 0\n",
    "    res = MetricScores()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (seq, tgt) in enumerate(tqdm(test_dataloader)):\n",
    "            y_trues = []\n",
    "            seq = seq.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            out = model(seq)\n",
    "            pred = torch.argsort(out[:, -1, :], dim=-1, descending=True)\n",
    "\n",
    "            tgt = tgt[:, -1]\n",
    "\n",
    "            res(tgt, pred)\n",
    "\n",
    "    return res.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, opt, loss_func, max_item):\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_iter = tqdm(train_dataloader, ncols=100)\n",
    "\n",
    "        for idx, (seq, tgt) in enumerate(train_iter):\n",
    "            seq = seq.to(args.device)\n",
    "            tgt = tgt.to(args.device)\n",
    "            \n",
    "            out = model(seq)\n",
    "            loss = loss_func(out.view(-1, max_item), tgt.view(-1))\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "            train_iter.set_postfix({\"loss\": total_loss / (idx + 1)})\n",
    "        \n",
    "        res = evaluate(model, test_dataloader, max_item, args.device)\n",
    "\n",
    "\n",
    "model = model.to(args.device)\n",
    "CE = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "train(model, train_dataloader, test_dataloader, opt, CE, max_item+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prom-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4b6f706935950af0f06eb70d2f98bec365628ed112ea68505c37d73a9c7e832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
